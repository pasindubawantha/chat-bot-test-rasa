{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FllyYWZrY8f0"
   },
   "source": [
    "## Building Your Own Chatbot: The Hard Way Made Easy \n",
    "\n",
    "\n",
    "---\n",
    "<img src=\"https://miro.medium.com/max/800/1*CKjk4H7XnYghNmau6wGU9g.jpeg\" alt=\"Drawing\" width=\"45%\"/>\n",
    "\n",
    "\n",
    "In this workshop, you will learn how to build your own conversational AI assistant using machine learning and real conversational data. The goal of this workshop is to walk you through the process of building an ML-powered assistant from scratch and build an actual assistant which you can improve later.\n",
    "\n",
    "\n",
    "There are no additional requirements to run this notebook. But if you encounter any issues or have more questions about the content included here, feel free to send a message to following email address at gamagebimsara@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7D42AfuY8f4"
   },
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5AYKRcpKY8f8"
   },
   "source": [
    "During the course of this 3-hour workshop, you will go through each stage of the chatbot development and build an assistant capable of providing real-time weather information of a given location. Below is an example conversation your assistant will be able to handle:\n",
    "\n",
    "U: Hello  \n",
    "A: Hey, how can I help?  \n",
    "U: What's the weather outside?  \n",
    "A: Where are you based?  \n",
    "U: Tell me the current weather in Colombo.  \n",
    "A: Sure, it is currently rainy in Colombo.   \n",
    "U: Goodbye.\n",
    "\n",
    "A: Goodbye :(\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ymUGMEcRY8f_"
   },
   "source": [
    "The workshop consists of the following stages:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**0. Intro:**  \n",
    "\n",
    "   0.1 Setup and installation \n",
    "      \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**1. Stage 1: Natural language understanding:** \n",
    "\n",
    "   1.1. Designing the happy path  \n",
    "    1.2. Generating the NLU training examples  \n",
    "    1.3. Designing the training pipeline  \n",
    "    \n",
    "      \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**2. Stage 2: Dialogue management model:** \n",
    "    \n",
    " 2.1. Designing training stories  \n",
    "    2.2. Creating a custom action  \n",
    "    2.3. Defining the domain  \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "      \n",
    "**3. Stage 3: Training and testing the model:**  \n",
    "   \n",
    "  3.1. Training the bot   \n",
    "    3.2. Testing the bot in the terminal  \n",
    "    3.3. Model evaluation\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**4. Stage 4: Closing the feedback loop:** \n",
    "   \n",
    "   4.1. Improving the assistant using the interactive learning\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "      \n",
    "**5. Stage 5: Integrating the chatbot with Slack:**  \n",
    "   \n",
    "   5.1. Setup the credentials   \n",
    "    5.2. Ngrok for local testing    \n",
    "    5.3. Start the rasa core server \n",
    "    \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mcjuA1LQY8gD"
   },
   "source": [
    "## 0. Intro\n",
    "In this section, we will install all the necessary dependencies needed to successfully run this exercise.\n",
    "### 0.1. Setup and installation\n",
    "The best way to insall the necessary modules is to use the requirements.txt file. After creating a virtual environment, run:\n",
    "\n",
    "**pip install -r requirements.txt**\n",
    "\n",
    "Throughout this workshop, we will use only open source tools. The code block below checks if Rasa NLU and Rasa Core have been installed successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'requirements' (str) to file 'requirements.txt'.\n"
     ]
    }
   ],
   "source": [
    "requirements = \"\"\"\n",
    "rasa[spacy]\n",
    "git+https://github.com/apixu/apixu-python.git\n",
    "ngrok\n",
    "\"\"\"\n",
    "%store requirements > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/apixu/apixu-python.git (from -r requirements.txt (line 3))\n",
      "  Cloning https://github.com/apixu/apixu-python.git to /tmp/pip-req-build-l_7feu28\n",
      "  Running command git clone -q https://github.com/apixu/apixu-python.git /tmp/pip-req-build-l_7feu28\n",
      "Collecting rasa[spacy] (from -r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/17/1d/61c091251bef9f8d49bb32b5f09625d0d0e2c32f22ffe4bf23937580caf9/rasa-1.2.5-py3-none-any.whl\n",
      "Collecting ngrok (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/fd/56/8245c77e761e13a376ed0b1d696c4e36b3126c5cdfb4fccbfe6ca41870e4/ngrok-0.0.1.tar.gz\n",
      "Collecting requests==2.21 (from apixu==0.3.0->-r requirements.txt (line 3))\n",
      "  Using cached https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl\n",
      "Collecting boto3~=1.9 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/1f/59aa653d8eb71060fa776c773d84bfafebe49cee1b041cd5a8899c32b9d8/boto3-1.9.218-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 154kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-socketio~=4.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/35/b0/22c3f785f23fec5c7a815f47c55d7e7946a67ae2129ff604148e939d3bdb/python_socketio-4.3.1-py2.py3-none-any.whl\n",
      "Collecting python-telegram-bot~=11.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/84/6c/47932a4041ee76650ad1f45a80e1422077e1e99c08a4d7a61cfbe5393d41/python_telegram_bot-11.1.0-py2.py3-none-any.whl\n",
      "Collecting scikit-learn~=0.20.2 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/87/6f/5863f1b27523c5d9f0ae2f3d07828ad383ceab39c79726d2ea4da7f679e7/scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: python-dateutil~=2.8 in /home/bimsara/anaconda3/envs/icter19/lib/python3.7/site-packages (from rasa[spacy]->-r requirements.txt (line 2)) (2.8.0)\n",
      "Collecting tensorflow~=1.13.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/70/45d3b9fab768215a2055c7819d39547a4b0b7401b4583094068741aff99b/tensorflow-1.13.2-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting pydot~=1.4 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/33/d1/b1479a770f66d962f545c2101630ce1d5592d90cb4f083d38862e93d16d2/pydot-1.4.1-py2.py3-none-any.whl\n",
      "Collecting async-generator~=1.10 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/71/52/39d20e03abd0ac9159c162ec24b93fbcaa111e8400308f2465432495ca2b/async_generator-1.10-py3-none-any.whl\n",
      "Collecting fakeredis~=1.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/77/35d9036794a87f12de587ae426cb74993ab686fb2fcca343d37a51f4bef6/fakeredis-1.0.4-py2.py3-none-any.whl\n",
      "Collecting scipy~=1.2 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/7f/b535ec711cbcc3246abea4385d17e1b325d4c3404dd86f15fc4f3dba1dbb/scipy-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (25.2MB)\n",
      "\u001b[K     |████████████████████████████████| 25.2MB 640kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting colorclass~=2.2 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "Collecting sanic-jwt~=1.3 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "Collecting jsonschema~=2.6 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/77/de/47e35a97b2b05c2fadbec67d44cfcdcd09b8086951b331d82de90d2912da/jsonschema-2.6.0-py2.py3-none-any.whl\n",
      "Collecting pymongo~=3.8 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/23/7666537adafcd232c88c156aa9382c859791d79bf12094005e009c2b6a3d/pymongo-3.9.0-cp37-cp37m-manylinux1_x86_64.whl (447kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 468kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rasa-sdk~=1.2.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/b0/76/21fc4991c18306adba67759fd9fff4cd453117f53731f45fa93fed02faa5/rasa_sdk-1.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: attrs>=18 in /home/bimsara/anaconda3/envs/icter19/lib/python3.7/site-packages (from rasa[spacy]->-r requirements.txt (line 2)) (19.1.0)\n",
      "Collecting tqdm~=4.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/88/d3213e2f3492daf09d8b41631ad6899f56db17ce83ea9c8a579902bafe5e/tqdm-4.35.0-py2.py3-none-any.whl (50kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 771kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy~=1.16 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/eb/4ecf6b13897391cb07a4231e9d9c671b55dfbbf6f4a514a1a0c594f2d8d9/numpy-1.17.1-cp37-cp37m-manylinux1_x86_64.whl (20.3MB)\n",
      "\u001b[K     |████████████████████████████████| 20.3MB 98kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting fbmessenger~=6.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/bd/e9/646684226176782b9e3b7dd5b35d7ecfd1d13cba24ad2e33255079921aab/fbmessenger-6.0.0-py2.py3-none-any.whl\n",
      "Collecting aiohttp~=3.5 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/33/60/c21acb7002110699761d3dec36fad3f5087c2752a4eb495444f877db6553/aiohttp-3.5.4-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting jsonpickle~=1.1 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
      "Collecting sanic-cors~=0.9.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/a9/7168016496ab945b6bad51deaa6e9c0526ad8b09a43db49561b8c82e5484/Sanic_Cors-0.9.9.post1-py2.py3-none-any.whl\n",
      "Collecting simplejson~=3.16 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "Collecting kafka-python~=1.4 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/82/39/aebe3ad518513bbb2260dd84ac21e5c30af860cc4c95b32acbd64b9d9d0d/kafka_python-1.4.6-py2.py3-none-any.whl\n",
      "Collecting questionary>=1.1.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/f9/8a6e7fce60566b3bcdc5ad0923916f38a65bca630ce3647251e672308bdf/questionary-1.3.0-py3-none-any.whl\n",
      "Collecting SQLAlchemy~=1.3.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/49/82d64d705ced344ba458197dadab30cfa745f9650ee22260ac2b275d288c/SQLAlchemy-1.3.8.tar.gz (5.9MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9MB 319kB/s eta 0:00:01     |█████████████████████▎          | 4.0MB 256kB/s eta 0:00:08\n",
      "\u001b[?25hCollecting networkx~=2.3 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "Collecting matplotlib~=3.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/19/7a/60bd79c5d79559150f8bba866dd7d434f0a170312e4d15e8aefa5faba294/matplotlib-3.1.1-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting sklearn-crfsuite~=0.3.6 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
      "Collecting sanic~=19.3.1 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/90/11/31382617b33f89df0caca396f104628d256689a58a63c75d3e46663a8c8f/sanic-19.3.1-py3-none-any.whl\n",
      "Collecting rocketchat-API~=0.6.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/e2/36/29d991028e2943b6a12abe12e3114a4cce13872283df7c7b784ee4cb59ed/rocketchat_API-0.6.34-py3-none-any.whl\n",
      "Collecting mattermostwrapper~=2.0 (from rasa[spacy]->-r requirements.txt (line 2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using cached https://files.pythonhosted.org/packages/93/70/203660597d12788e958dd691aa11c3c29caa075eadb2ce94d2eb53099d1b/mattermostwrapper-2.1-py2.py3-none-any.whl\n",
      "Collecting terminaltables~=3.1 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "Collecting colorhash~=1.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/0e/e1/50dbc513aa74e99eca4c47f2a8206711f0bec436fdddd95eebaf7eaaa1aa/colorhash-1.0.2-py2.py3-none-any.whl\n",
      "Collecting pytz~=2019.1 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/87/76/46d697698a143e05f77bec5a526bf4e56a0be61d63425b68f4ba553b51f2/pytz-2019.2-py2.py3-none-any.whl\n",
      "Collecting redis~=3.3.5 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/64/b1e90af9bf0c7f6ef55e46b81ab527b33b785824d65300bb65636534b530/redis-3.3.8-py2.py3-none-any.whl (66kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 217kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting apscheduler~=3.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/09/ff/d5b0e81846cd5e92d02e5f2682b78c73a5d9d61bc1eae32cea5ac15c0d47/APScheduler-3.6.1-py2.py3-none-any.whl\n",
      "Collecting gevent~=1.4 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/89/57b63d6d7967d8763b913172bf6831afb01951b9ed9da127f2938a365585/gevent-1.4.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting coloredlogs~=10.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/08/0f/7877fc42fff0b9d70b6442df62d53b3868d3a6ad1b876bdb54335b30ff23/coloredlogs-10.0-py2.py3-none-any.whl\n",
      "Collecting twilio~=6.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/01/0e06bdbce5aa6e7d698ce4b2e086faf13d00a8dc9a95c64836544710a47f/twilio-6.29.4-py2.py3-none-any.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 567kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting packaging~=19.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/ec/22/630ac83e8f8a9566c4f88038447ed9e16e6f10582767a01f31c769d9a71e/packaging-19.1-py2.py3-none-any.whl\n",
      "Collecting pika~=1.0.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/78/1a/28c98ee8b211be21d4a9f4ef1687c4d36f9302d47fcc28b81f9591abf6d8/pika-1.0.1-py2.py3-none-any.whl\n",
      "Collecting webexteamssdk~=1.1 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "Collecting ruamel.yaml~=0.15.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/71/fc/12de89822adaa3a60b8cb0139bae75918278999d08e6dff158623abd7cba/ruamel.yaml-0.15.100-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting slackclient~=1.3 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "Collecting pykwalify~=1.7.0 (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/36/9f/612de8ca540bd24d604f544248c4c46e9db76f6ea5eb75fb4244da6ebbf0/pykwalify-1.7.0-py2.py3-none-any.whl\n",
      "Collecting spacy<2.2,>=2.1; extra == \"spacy\" (from rasa[spacy]->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/75/3c000560b15248530694b4bf6222357549accf24b9ee5b27a3f0acc8323e/spacy-2.1.8-cp37-cp37m-manylinux1_x86_64.whl (30.8MB)\n",
      "\u001b[K     |████████████████████████████████| 30.8MB 756kB/s eta 0:00:011   |████                            | 3.9MB 650kB/s eta 0:00:42     |██████████                      | 9.6MB 197kB/s eta 0:01:48     |██████████████████████████████▎ | 29.2MB 424kB/s eta 0:00:04\n",
      "\u001b[?25hCollecting chardet<3.1.0,>=3.0.2 (from requests==2.21->apixu==0.3.0->-r requirements.txt (line 3))\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting urllib3<1.25,>=1.21.1 (from requests==2.21->apixu==0.3.0->-r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/11/525b02e4acc0c747de8b6ccdab376331597c569c42ea66ab0a1dbd36eca2/urllib3-1.24.3-py2.py3-none-any.whl (118kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 271kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/bimsara/anaconda3/envs/icter19/lib/python3.7/site-packages (from requests==2.21->apixu==0.3.0->-r requirements.txt (line 3)) (2019.6.16)\n",
      "Collecting idna<2.9,>=2.5 (from requests==2.21->apixu==0.3.0->-r requirements.txt (line 3))\n",
      "  Using cached https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3~=1.9->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from boto3~=1.9->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl\n",
      "Collecting botocore<1.13.0,>=1.12.218 (from boto3~=1.9->rasa[spacy]->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/aa/359ceab0a3c3fdc257bcffb09eee8335ecf9c673228255fa64ef228fcbfd/botocore-1.12.218-py2.py3-none-any.whl (5.7MB)\n",
      "\u001b[K     |████████████████████████████████| 5.7MB 736kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /home/bimsara/anaconda3/envs/icter19/lib/python3.7/site-packages (from python-socketio~=4.0->rasa[spacy]->-r requirements.txt (line 2)) (1.12.0)\n",
      "Collecting python-engineio>=3.9.0 (from python-socketio~=4.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/2b/20/8e3ba16102ae2e245d70d9cb9fa48b076253fdb036dc43eea142294c2897/python_engineio-3.9.3-py2.py3-none-any.whl\n",
      "Collecting cryptography (from python-telegram-bot~=11.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/97/18/c6557f63a6abde34707196fb2cad1c6dc0dbff25a200d5044922496668a4/cryptography-2.7-cp34-abi3-manylinux1_x86_64.whl\n",
      "Collecting future>=0.16.0 (from python-telegram-bot~=11.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow~=1.13.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl\n",
      "Collecting grpcio>=1.8.6 (from tensorflow~=1.13.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/27/1f908ebb99c8d48a5ba4eb9d7997f5633b920d98fe712f67aaa0663f1307/grpcio-1.23.0-cp37-cp37m-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2MB 576kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /home/bimsara/anaconda3/envs/icter19/lib/python3.7/site-packages (from tensorflow~=1.13.0->rasa[spacy]->-r requirements.txt (line 2)) (0.33.4)\n",
      "Collecting astor>=0.6.0 (from tensorflow~=1.13.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow~=1.13.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow~=1.13.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
      "Collecting protobuf>=3.6.1 (from tensorflow~=1.13.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/30/fd/60ce148d8e4205bdf6da4ffec31348fd33f710c20a882b44319d54fd51ae/protobuf-3.9.1-cp37-cp37m-manylinux1_x86_64.whl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gast>=0.2.0 (from tensorflow~=1.13.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "Collecting absl-py>=0.1.6 (from tensorflow~=1.13.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/0d/7cbf64cac3f93617a2b6b079c0182e4a83a3e7a8964d3b0cc3d9758ba002/absl-py-0.8.0.tar.gz (102kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 636kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from tensorflow~=1.13.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\n",
      "Collecting termcolor>=1.1.0 (from tensorflow~=1.13.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "Collecting pyparsing>=2.1.4 (from pydot~=1.4->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/11/fa/0160cd525c62d7abd076a070ff02b2b94de589f1a9789774f17d7c54058e/pyparsing-2.4.2-py2.py3-none-any.whl\n",
      "Collecting sortedcontainers (from fakeredis~=1.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/13/f3/cf85f7c3a2dbd1a515d51e1f1676d971abe41bba6f4ab5443240d9a78e5b/sortedcontainers-2.1.0-py2.py3-none-any.whl\n",
      "Collecting pyjwt (from sanic-jwt~=1.3->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl\n",
      "Collecting flask~=1.0 (from rasa-sdk~=1.2.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/9b/93/628509b8d5dc749656a9641f4caf13540e2cdec85276964ff8f43bbb1d3b/Flask-1.1.1-py2.py3-none-any.whl\n",
      "Collecting ConfigArgParse~=0.14 (from rasa-sdk~=1.2.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "Collecting flask-cors~=3.0 (from rasa-sdk~=1.2.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp~=3.5->rasa[spacy]->-r requirements.txt (line 2))\n",
      "Collecting multidict<5.0,>=4.0 (from aiohttp~=3.5->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/88/f0/4d4cbd1a3744e3985efa49682352d0703df653ffa76b81f10fed86599a50/multidict-4.5.2-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting async-timeout<4.0,>=3.0 (from aiohttp~=3.5->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
      "Collecting sanic-plugins-framework>=0.8.2 (from sanic-cors~=0.9.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/ac/a6/258bdd353c22c3ff7f130d1c788f874a88e48d306614d2f622f3bac2576b/Sanic_Plugins_Framework-0.8.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: prompt-toolkit~=2.0 in /home/bimsara/anaconda3/envs/icter19/lib/python3.7/site-packages (from questionary>=1.1.0->rasa[spacy]->-r requirements.txt (line 2)) (2.0.9)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/bimsara/anaconda3/envs/icter19/lib/python3.7/site-packages (from networkx~=2.3->rasa[spacy]->-r requirements.txt (line 2)) (4.4.0)\n",
      "Collecting cycler>=0.10 (from matplotlib~=3.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib~=3.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/93/f8/518fb0bb89860eea6ff1b96483fbd9236d5ee991485d0f3eceff1770f654/kiwisolver-1.1.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting tabulate (from sklearn-crfsuite~=0.3.6->rasa[spacy]->-r requirements.txt (line 2))\n",
      "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite~=0.3.6->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/da/05/5cd3eb8dbbe3c787e3cf84d5767d95198298f7951bf8e40c46ebd8c80a32/python_crfsuite-0.9.6-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting websockets<7.0,>=6.0 (from sanic~=19.3.1->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/0c/6a/d048dfe820fc956e57bb1115f5eda5a1bef320172811f72c9924c8d6ebb5/websockets-6.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting httptools>=0.0.10 (from sanic~=19.3.1->rasa[spacy]->-r requirements.txt (line 2))\n",
      "Collecting ujson>=1.35; sys_platform != \"win32\" and implementation_name == \"cpython\" (from sanic~=19.3.1->rasa[spacy]->-r requirements.txt (line 2))\n",
      "Collecting uvloop>=0.5.3; sys_platform != \"win32\" and implementation_name == \"cpython\" (from sanic~=19.3.1->rasa[spacy]->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/27/83c42973ae0a27340c32170fa261f1692319371df1fb3bf054a00dd52d7c/uvloop-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 297kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiofiles>=0.3.0 (from sanic~=19.3.1->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/cf/f2/a67a23bc0bb61d88f82aa7fb84a2fb5f278becfbdc038c5cbb36c31feaf1/aiofiles-0.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: setuptools>=0.7 in /home/bimsara/anaconda3/envs/icter19/lib/python3.7/site-packages (from apscheduler~=3.0->rasa[spacy]->-r requirements.txt (line 2)) (41.0.1)\n",
      "Collecting tzlocal>=1.2 (from apscheduler~=3.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/ef/99/53bd1ac9349262f59c1c421d8fcc2559ae8a5eeffed9202684756b648d33/tzlocal-2.0.0-py2.py3-none-any.whl\n",
      "Collecting greenlet>=0.4.14; platform_python_implementation == \"CPython\" (from gevent~=1.4->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/9d/ef/ac10aa1293f64939e4511909c570d969566126214af5dd7ba0afd353d88b/greenlet-0.4.15-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting humanfriendly>=4.7 (from coloredlogs~=10.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/90/df/88bff450f333114680698dc4aac7506ff7cab164b794461906de31998665/humanfriendly-4.18-py2.py3-none-any.whl\n",
      "Collecting pysocks; python_version >= \"3.0\" (from twilio~=6.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/cd/18/102cc70347486e75235a29a6543f002cf758042189cb063ec25334993e36/PySocks-1.7.0-py3-none-any.whl\n",
      "Collecting requests-toolbelt (from webexteamssdk~=1.1->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/60/ef/7681134338fc097acef8d9b2f8abe0458e4d87559c689a8c306d0957ece5/requests_toolbelt-0.9.1-py2.py3-none-any.whl\n",
      "Collecting websocket-client<0.55.0,>=0.35 (from slackclient~=1.3->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/26/2d/f749a5c82f6192d77ed061a38e02001afcba55fe8477336d26a950ab17ce/websocket_client-0.54.0-py2.py3-none-any.whl\n",
      "Collecting PyYAML>=3.11 (from pykwalify~=1.7.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "Collecting docopt>=0.6.2 (from pykwalify~=1.7.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy<2.2,>=2.1; extra == \"spacy\"->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fc/10eeacb926ec1e88cd62f79d9ac106b0a3e3fe5ff1690422d88c29bd0909/murmurhash-1.0.2-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy<2.2,>=2.1; extra == \"spacy\"->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/65/26/e534148e509cbebbea3ee29f50f59eb206621d12c35e4594507da8dc54cc/cymem-2.0.2-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting thinc<7.1.0,>=7.0.8 (from spacy<2.2,>=2.1; extra == \"spacy\"->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/36/42/d7ea7539af3852fd8c1f0b3adf4a100fb3d72b40b69cef1a764ff979a743/thinc-7.0.8-cp37-cp37m-manylinux1_x86_64.whl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting srsly<1.1.0,>=0.0.6 (from spacy<2.2,>=2.1; extra == \"spacy\"->rasa[spacy]->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/a3/8d84ede325d26075a4e1e2cba01201c6301545bca96dbff60ab9e9d96c3e/srsly-0.1.0-cp37-cp37m-manylinux1_x86_64.whl (181kB)\n",
      "\u001b[K     |████████████████████████████████| 184kB 614kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting preshed<2.1.0,>=2.0.1 (from spacy<2.2,>=2.1; extra == \"spacy\"->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/2b/3ecd5d90d2d6fd39fbc520de7d80db5d74defdc2d7c2e15531d9cc3498c7/preshed-2.0.1-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting wasabi<1.1.0,>=0.2.0 (from spacy<2.2,>=2.1; extra == \"spacy\"->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/f4/c1/d76ccdd12c716be79162d934fe7de4ac8a318b9302864716dde940641a79/wasabi-0.2.2-py3-none-any.whl\n",
      "Collecting plac<1.0.0,>=0.9.6 (from spacy<2.2,>=2.1; extra == \"spacy\"->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
      "Collecting blis<0.3.0,>=0.2.2 (from spacy<2.2,>=2.1; extra == \"spacy\"->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/fa/5f/47b7b29ad202b2210020e2f33bfb06d1db2abe0e709c2a84736e8a9d1bd5/blis-0.2.4-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting docutils<0.16,>=0.10 (from botocore<1.13.0,>=1.12.218->boto3~=1.9->rasa[spacy]->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\n",
      "\u001b[K     |████████████████████████████████| 552kB 506kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting asn1crypto>=0.21.0 (from cryptography->python-telegram-bot~=11.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl\n",
      "Collecting cffi!=1.11.3,>=1.8 (from cryptography->python-telegram-bot~=11.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/a0/ea/37fe21475c884f88a2ae496cab10e8f84f0cc11137be860af9eb37a3edb9/cffi-1.12.3-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow~=1.13.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow~=1.13.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard<1.14.0,>=1.13.0->tensorflow~=1.13.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/ab/d3bed6b92042622d24decc7aadc8877badf18aeca1571045840ad4956d3f/Werkzeug-0.15.5-py2.py3-none-any.whl\n",
      "Collecting h5py (from keras-applications>=1.0.6->tensorflow~=1.13.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/8e/fd/2ca5c4f4ed33ac4178f9c4d551e3946ab480866e3cd67a65a67a4bb35367/h5py-2.9.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting click>=5.1 (from flask~=1.0->rasa-sdk~=1.2.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl\n",
      "Collecting itsdangerous>=0.24 (from flask~=1.0->rasa-sdk~=1.2.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /home/bimsara/anaconda3/envs/icter19/lib/python3.7/site-packages (from flask~=1.0->rasa-sdk~=1.2.0->rasa[spacy]->-r requirements.txt (line 2)) (2.10.1)\n",
      "Requirement already satisfied: wcwidth in /home/bimsara/anaconda3/envs/icter19/lib/python3.7/site-packages (from prompt-toolkit~=2.0->questionary>=1.1.0->rasa[spacy]->-r requirements.txt (line 2)) (0.1.7)\n",
      "Collecting pycparser (from cffi!=1.11.3,>=1.8->cryptography->python-telegram-bot~=11.0->rasa[spacy]->-r requirements.txt (line 2))\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/bimsara/anaconda3/envs/icter19/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask~=1.0->rasa-sdk~=1.2.0->rasa[spacy]->-r requirements.txt (line 2)) (1.1.1)\n",
      "Building wheels for collected packages: ngrok, apixu, SQLAlchemy, absl-py\n",
      "  Building wheel for ngrok (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ngrok: filename=ngrok-0.0.1-cp37-none-any.whl size=3061 sha256=5fcdfc8e5ce1d2075d03e351c822253aeff8e5f2a514f97e746792830588fc3f\n",
      "  Stored in directory: /home/bimsara/.cache/pip/wheels/7c/e5/bb/0aba6d734b1363ac037c3806ba9c5af55cc0f5f6d711153813\n",
      "  Building wheel for apixu (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for apixu: filename=apixu-0.3.0-cp37-none-any.whl size=3444 sha256=39b8fb30647210d12539de4f8c1b2d8374f91247078c1af1ab8afb9a87728590\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ewqqyfu4/wheels/28/02/28/4afa0ebeda281d4d4ea2736a39fdbaf533aefa8615e0b52230\n",
      "  Building wheel for SQLAlchemy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for SQLAlchemy: filename=SQLAlchemy-1.3.8-cp37-cp37m-linux_x86_64.whl size=1198063 sha256=9c8c8fe67c77ffc7e12f8b4c40585b27ef453ba9d14aa79491ac741a132ac3c4\n",
      "  Stored in directory: /home/bimsara/.cache/pip/wheels/97/b6/66/de2064d40c920adc2984ff3b8fd4f11494c8ab9e48ba87e8a2\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.8.0-cp37-none-any.whl size=120986 sha256=0c9f1bf8335f80d87e9db49433b60422da40e9238e72c95c90b97fbdd253fa42\n",
      "  Stored in directory: /home/bimsara/.cache/pip/wheels/9a/1e/7a/456008eb5e47fd5de792c6139df6d5b3d5f71d51c6a0b94799\n",
      "Successfully built ngrok apixu SQLAlchemy absl-py\n",
      "\u001b[31mERROR: rasa-sdk 1.2.0 has requirement requests~=2.22, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: jmespath, urllib3, docutils, botocore, s3transfer, boto3, python-engineio, python-socketio, asn1crypto, pycparser, cffi, cryptography, future, python-telegram-bot, numpy, scipy, scikit-learn, absl-py, mock, tensorflow-estimator, grpcio, astor, protobuf, markdown, werkzeug, tensorboard, h5py, keras-applications, gast, keras-preprocessing, termcolor, tensorflow, pyparsing, pydot, async-generator, sortedcontainers, redis, fakeredis, colorclass, pyjwt, sanic-jwt, jsonschema, pymongo, greenlet, gevent, humanfriendly, coloredlogs, click, itsdangerous, flask, chardet, idna, requests, ConfigArgParse, flask-cors, rasa-sdk, tqdm, fbmessenger, multidict, yarl, async-timeout, aiohttp, jsonpickle, websockets, httptools, ujson, uvloop, aiofiles, sanic, sanic-plugins-framework, sanic-cors, simplejson, kafka-python, questionary, SQLAlchemy, networkx, cycler, kiwisolver, matplotlib, tabulate, python-crfsuite, sklearn-crfsuite, rocketchat-API, mattermostwrapper, terminaltables, colorhash, pytz, tzlocal, apscheduler, pysocks, twilio, packaging, pika, requests-toolbelt, webexteamssdk, ruamel.yaml, websocket-client, slackclient, PyYAML, docopt, pykwalify, murmurhash, cymem, blis, wasabi, preshed, srsly, plac, thinc, spacy, rasa, ngrok, apixu\n",
      "  Found existing installation: jsonschema 3.0.2\n",
      "    Uninstalling jsonschema-3.0.2:\n",
      "      Successfully uninstalled jsonschema-3.0.2\n",
      "Successfully installed ConfigArgParse-0.14.0 PyYAML-5.1.2 SQLAlchemy-1.3.8 absl-py-0.8.0 aiofiles-0.4.0 aiohttp-3.5.4 apixu-0.3.0 apscheduler-3.6.1 asn1crypto-0.24.0 astor-0.8.0 async-generator-1.10 async-timeout-3.0.1 blis-0.2.4 boto3-1.9.218 botocore-1.12.218 cffi-1.12.3 chardet-3.0.4 click-7.0 colorclass-2.2.0 coloredlogs-10.0 colorhash-1.0.2 cryptography-2.7 cycler-0.10.0 cymem-2.0.2 docopt-0.6.2 docutils-0.15.2 fakeredis-1.0.4 fbmessenger-6.0.0 flask-1.1.1 flask-cors-3.0.8 future-0.17.1 gast-0.2.2 gevent-1.4.0 greenlet-0.4.15 grpcio-1.23.0 h5py-2.9.0 httptools-0.0.13 humanfriendly-4.18 idna-2.8 itsdangerous-1.1.0 jmespath-0.9.4 jsonpickle-1.2 jsonschema-2.6.0 kafka-python-1.4.6 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 markdown-3.1.1 matplotlib-3.1.1 mattermostwrapper-2.1 mock-3.0.5 multidict-4.5.2 murmurhash-1.0.2 networkx-2.3 ngrok-0.0.1 numpy-1.17.1 packaging-19.1 pika-1.0.1 plac-0.9.6 preshed-2.0.1 protobuf-3.9.1 pycparser-2.19 pydot-1.4.1 pyjwt-1.7.1 pykwalify-1.7.0 pymongo-3.9.0 pyparsing-2.4.2 pysocks-1.7.0 python-crfsuite-0.9.6 python-engineio-3.9.3 python-socketio-4.3.1 python-telegram-bot-11.1.0 pytz-2019.2 questionary-1.3.0 rasa-1.2.5 rasa-sdk-1.2.0 redis-3.3.8 requests-2.21.0 requests-toolbelt-0.9.1 rocketchat-API-0.6.34 ruamel.yaml-0.15.100 s3transfer-0.2.1 sanic-19.3.1 sanic-cors-0.9.9.post1 sanic-jwt-1.3.2 sanic-plugins-framework-0.8.2 scikit-learn-0.20.4 scipy-1.3.1 simplejson-3.16.0 sklearn-crfsuite-0.3.6 slackclient-1.3.2 sortedcontainers-2.1.0 spacy-2.1.8 srsly-0.1.0 tabulate-0.8.3 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0 termcolor-1.1.0 terminaltables-3.1.0 thinc-7.0.8 tqdm-4.35.0 twilio-6.29.4 tzlocal-2.0.0 ujson-1.35 urllib3-1.24.3 uvloop-0.13.0 wasabi-0.2.2 webexteamssdk-1.1.1 websocket-client-0.54.0 websockets-6.0 werkzeug-0.15.5 yarl-1.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rasa_nlu: 1.2.5 rasa_core: 1.2.5\n"
     ]
    }
   ],
   "source": [
    "import rasa.nlu\n",
    "import rasa.core\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "print(\"rasa_nlu: {} rasa_core: {}\".format(rasa.nlu.__version__, rasa.core.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should also download **Spacy's English language model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_md==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz#egg=en_core_web_md==2.1.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n",
      "\u001b[K     |████████████████████████████████| 95.4MB 356kB/s eta 0:00:01     |███████████████████████████████▍| 93.6MB 258kB/s eta 0:00:07\n",
      "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
      "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-cp37-none-any.whl size=97126237 sha256=fb1354557d126f50ce6c4aaaee66d280fb69176e29507fa2f1b754edb75f4c3c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-9lhano6s/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n",
      "Successfully built en-core-web-md\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\r\n",
      "/home/bimsara/anaconda3/envs/icter19/lib/python3.7/site-packages/en_core_web_md\r\n",
      "-->\r\n",
      "/home/bimsara/anaconda3/envs/icter19/lib/python3.7/site-packages/spacy/data/en\r\n",
      "You can now load the model via spacy.load('en')\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy link en_core_web_md en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change the working directory** to a newly created one where the necessary project files should be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bimsara/Desktop/icter19/bot\n"
     ]
    }
   ],
   "source": [
    "%cd bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bimsara/Desktop/icter19/bot\r\n"
     ]
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create **a new Rasa project**. To do this, run:\n",
    "\n",
    "**rasa init --no-prompt**\n",
    "\n",
    "The **rasa init** command creates all the files that a Rasa project needs and trains a simple bot on some sample data. If you leave out the **--no-prompt** flag you will be asked some questions about how you want your project to be set up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bimsara/anaconda3/envs/icter19/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/bimsara/anaconda3/envs/icter19/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/bimsara/anaconda3/envs/icter19/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/bimsara/anaconda3/envs/icter19/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/bimsara/anaconda3/envs/icter19/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/bimsara/anaconda3/envs/icter19/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[92mWelcome to Rasa! 🤖\n",
      "\u001b[0m\n",
      "To get started quickly, an initial project will be created.\n",
      "If you need some help, check out the documentation at https://rasa.com/docs/rasa.\n",
      "\n",
      "Created project directory at '/home/bimsara/Desktop/icter19/bot'.\n",
      "\u001b[92mFinished creating project structure.\u001b[0m\n",
      "\u001b[92mTraining an initial model...\u001b[0m\n",
      "\u001b[94mTraining Core model...\u001b[0m\n",
      "Processed Story Blocks: 100%|█████| 4/4 [00:00<00:00, 3385.92it/s, # trackers=1]\n",
      "Processed Story Blocks: 100%|█████| 4/4 [00:00<00:00, 1418.31it/s, # trackers=4]\n",
      "Processed Story Blocks: 100%|█████| 4/4 [00:00<00:00, 451.96it/s, # trackers=12]\n",
      "Processed Story Blocks: 100%|█████| 4/4 [00:00<00:00, 520.85it/s, # trackers=10]\n",
      "Processed trackers: 100%|█████████| 4/4 [00:00<00:00, 2579.52it/s, # actions=14]\n",
      "Processed actions: 14it [00:00, 9981.35it/s, # examples=14]\n",
      "Processed trackers: 100%|██████| 100/100 [00:00<00:00, 925.91it/s, # actions=62]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, 5, 19)             0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                6656      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 13)                429       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 13)                0         \n",
      "=================================================================\n",
      "Total params: 7,085\n",
      "Trainable params: 7,085\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2019-08-29 20:51:31 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.core.policies.keras_policy\u001b[0m  - Fitting model with 62 total samples and a validation split of 0.1\n",
      "Epoch 1/100\n",
      "62/62 [==============================] - 1s 13ms/sample - loss: 2.5010 - acc: 0.1935\n",
      "Epoch 2/100\n",
      "62/62 [==============================] - 0s 223us/sample - loss: 2.4503 - acc: 0.3226\n",
      "Epoch 3/100\n",
      "62/62 [==============================] - 0s 239us/sample - loss: 2.4116 - acc: 0.4032\n",
      "Epoch 4/100\n",
      "62/62 [==============================] - 0s 220us/sample - loss: 2.3997 - acc: 0.4032\n",
      "Epoch 5/100\n",
      "62/62 [==============================] - 0s 277us/sample - loss: 2.3598 - acc: 0.4194\n",
      "Epoch 6/100\n",
      "62/62 [==============================] - 0s 255us/sample - loss: 2.3118 - acc: 0.4355\n",
      "Epoch 7/100\n",
      "62/62 [==============================] - 0s 246us/sample - loss: 2.2774 - acc: 0.4355\n",
      "Epoch 8/100\n",
      "62/62 [==============================] - 0s 300us/sample - loss: 2.2414 - acc: 0.4355\n",
      "Epoch 9/100\n",
      "62/62 [==============================] - 0s 283us/sample - loss: 2.2154 - acc: 0.4355\n",
      "Epoch 10/100\n",
      "62/62 [==============================] - 0s 271us/sample - loss: 2.1729 - acc: 0.4355\n",
      "Epoch 11/100\n",
      "62/62 [==============================] - 0s 243us/sample - loss: 2.1267 - acc: 0.4355\n",
      "Epoch 12/100\n",
      "62/62 [==============================] - 0s 251us/sample - loss: 2.1114 - acc: 0.4355\n",
      "Epoch 13/100\n",
      "62/62 [==============================] - 0s 260us/sample - loss: 2.0474 - acc: 0.4355\n",
      "Epoch 14/100\n",
      "62/62 [==============================] - 0s 236us/sample - loss: 2.0404 - acc: 0.4355\n",
      "Epoch 15/100\n",
      "62/62 [==============================] - 0s 241us/sample - loss: 1.9750 - acc: 0.4355\n",
      "Epoch 16/100\n",
      "62/62 [==============================] - 0s 253us/sample - loss: 1.9548 - acc: 0.4355\n",
      "Epoch 17/100\n",
      "62/62 [==============================] - 0s 247us/sample - loss: 1.9114 - acc: 0.4355\n",
      "Epoch 18/100\n",
      "62/62 [==============================] - 0s 261us/sample - loss: 1.8936 - acc: 0.4355\n",
      "Epoch 19/100\n",
      "62/62 [==============================] - 0s 232us/sample - loss: 1.8337 - acc: 0.4355\n",
      "Epoch 20/100\n",
      "62/62 [==============================] - 0s 257us/sample - loss: 1.8030 - acc: 0.4355\n",
      "Epoch 21/100\n",
      "62/62 [==============================] - 0s 274us/sample - loss: 1.7949 - acc: 0.4355\n",
      "Epoch 22/100\n",
      "62/62 [==============================] - 0s 242us/sample - loss: 1.7822 - acc: 0.4355\n",
      "Epoch 23/100\n",
      "62/62 [==============================] - 0s 257us/sample - loss: 1.7452 - acc: 0.4355\n",
      "Epoch 24/100\n",
      "62/62 [==============================] - 0s 248us/sample - loss: 1.7687 - acc: 0.4355\n",
      "Epoch 25/100\n",
      "62/62 [==============================] - 0s 283us/sample - loss: 1.7312 - acc: 0.4355\n",
      "Epoch 26/100\n",
      "62/62 [==============================] - 0s 247us/sample - loss: 1.6993 - acc: 0.4355\n",
      "Epoch 27/100\n",
      "62/62 [==============================] - 0s 244us/sample - loss: 1.7022 - acc: 0.4355\n",
      "Epoch 28/100\n",
      "62/62 [==============================] - 0s 285us/sample - loss: 1.6822 - acc: 0.4355\n",
      "Epoch 29/100\n",
      "62/62 [==============================] - 0s 269us/sample - loss: 1.6594 - acc: 0.4355\n",
      "Epoch 30/100\n",
      "62/62 [==============================] - 0s 252us/sample - loss: 1.6467 - acc: 0.4355\n",
      "Epoch 31/100\n",
      "62/62 [==============================] - 0s 242us/sample - loss: 1.6483 - acc: 0.4355\n",
      "Epoch 32/100\n",
      "62/62 [==============================] - 0s 339us/sample - loss: 1.6496 - acc: 0.4355\n",
      "Epoch 33/100\n",
      "62/62 [==============================] - 0s 285us/sample - loss: 1.6380 - acc: 0.4355\n",
      "Epoch 34/100\n",
      "62/62 [==============================] - 0s 258us/sample - loss: 1.5935 - acc: 0.4355\n",
      "Epoch 35/100\n",
      "62/62 [==============================] - 0s 264us/sample - loss: 1.6005 - acc: 0.4355\n",
      "Epoch 36/100\n",
      "62/62 [==============================] - 0s 250us/sample - loss: 1.5813 - acc: 0.4355\n",
      "Epoch 37/100\n",
      "62/62 [==============================] - 0s 265us/sample - loss: 1.5629 - acc: 0.4355\n",
      "Epoch 38/100\n",
      "62/62 [==============================] - 0s 315us/sample - loss: 1.5688 - acc: 0.4355\n",
      "Epoch 39/100\n",
      "62/62 [==============================] - 0s 252us/sample - loss: 1.5496 - acc: 0.4355\n",
      "Epoch 40/100\n",
      "62/62 [==============================] - 0s 282us/sample - loss: 1.5661 - acc: 0.4355\n",
      "Epoch 41/100\n",
      "62/62 [==============================] - 0s 244us/sample - loss: 1.5446 - acc: 0.4355\n",
      "Epoch 42/100\n",
      "62/62 [==============================] - 0s 263us/sample - loss: 1.5281 - acc: 0.4355\n",
      "Epoch 43/100\n",
      "62/62 [==============================] - 0s 303us/sample - loss: 1.4894 - acc: 0.4355\n",
      "Epoch 44/100\n",
      "62/62 [==============================] - 0s 271us/sample - loss: 1.4813 - acc: 0.4355\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 468us/sample - loss: 1.4991 - acc: 0.4355\n",
      "Epoch 46/100\n",
      "62/62 [==============================] - 0s 347us/sample - loss: 1.4805 - acc: 0.4355\n",
      "Epoch 47/100\n",
      "62/62 [==============================] - 0s 376us/sample - loss: 1.4541 - acc: 0.4355\n",
      "Epoch 48/100\n",
      "62/62 [==============================] - 0s 377us/sample - loss: 1.4506 - acc: 0.4355\n",
      "Epoch 49/100\n",
      "62/62 [==============================] - 0s 294us/sample - loss: 1.4300 - acc: 0.4355\n",
      "Epoch 50/100\n",
      "62/62 [==============================] - 0s 527us/sample - loss: 1.4218 - acc: 0.4355\n",
      "Epoch 51/100\n",
      "62/62 [==============================] - 0s 374us/sample - loss: 1.4265 - acc: 0.4355\n",
      "Epoch 52/100\n",
      "62/62 [==============================] - 0s 341us/sample - loss: 1.4249 - acc: 0.4355\n",
      "Epoch 53/100\n",
      "62/62 [==============================] - 0s 345us/sample - loss: 1.4161 - acc: 0.4355\n",
      "Epoch 54/100\n",
      "62/62 [==============================] - 0s 283us/sample - loss: 1.4086 - acc: 0.4355\n",
      "Epoch 55/100\n",
      "62/62 [==============================] - 0s 256us/sample - loss: 1.4091 - acc: 0.4355\n",
      "Epoch 56/100\n",
      "62/62 [==============================] - 0s 289us/sample - loss: 1.3823 - acc: 0.4355\n",
      "Epoch 57/100\n",
      "62/62 [==============================] - 0s 241us/sample - loss: 1.3527 - acc: 0.4355\n",
      "Epoch 58/100\n",
      "62/62 [==============================] - 0s 267us/sample - loss: 1.3723 - acc: 0.4355\n",
      "Epoch 59/100\n",
      "62/62 [==============================] - 0s 287us/sample - loss: 1.3608 - acc: 0.4355\n",
      "Epoch 60/100\n",
      "62/62 [==============================] - 0s 246us/sample - loss: 1.3573 - acc: 0.4355\n",
      "Epoch 61/100\n",
      "62/62 [==============================] - 0s 222us/sample - loss: 1.3367 - acc: 0.4355\n",
      "Epoch 62/100\n",
      "62/62 [==============================] - 0s 241us/sample - loss: 1.3300 - acc: 0.4355\n",
      "Epoch 63/100\n",
      "62/62 [==============================] - 0s 252us/sample - loss: 1.3134 - acc: 0.4355\n",
      "Epoch 64/100\n",
      "62/62 [==============================] - 0s 221us/sample - loss: 1.3143 - acc: 0.4355\n",
      "Epoch 65/100\n",
      "62/62 [==============================] - 0s 242us/sample - loss: 1.3038 - acc: 0.4516\n",
      "Epoch 66/100\n",
      "62/62 [==============================] - 0s 252us/sample - loss: 1.2812 - acc: 0.4839\n",
      "Epoch 67/100\n",
      "62/62 [==============================] - 0s 222us/sample - loss: 1.3046 - acc: 0.4355\n",
      "Epoch 68/100\n",
      "62/62 [==============================] - 0s 246us/sample - loss: 1.2774 - acc: 0.4516\n",
      "Epoch 69/100\n",
      "62/62 [==============================] - 0s 262us/sample - loss: 1.2701 - acc: 0.4677\n",
      "Epoch 70/100\n",
      "62/62 [==============================] - 0s 215us/sample - loss: 1.2700 - acc: 0.4355\n",
      "Epoch 71/100\n",
      "62/62 [==============================] - 0s 254us/sample - loss: 1.2293 - acc: 0.5000\n",
      "Epoch 72/100\n",
      "62/62 [==============================] - 0s 326us/sample - loss: 1.2626 - acc: 0.4677\n",
      "Epoch 73/100\n",
      "62/62 [==============================] - 0s 310us/sample - loss: 1.2722 - acc: 0.4839\n",
      "Epoch 74/100\n",
      "62/62 [==============================] - 0s 332us/sample - loss: 1.2161 - acc: 0.5000\n",
      "Epoch 75/100\n",
      "62/62 [==============================] - 0s 308us/sample - loss: 1.2189 - acc: 0.5000\n",
      "Epoch 76/100\n",
      "62/62 [==============================] - 0s 341us/sample - loss: 1.2141 - acc: 0.4677\n",
      "Epoch 77/100\n",
      "62/62 [==============================] - 0s 316us/sample - loss: 1.2128 - acc: 0.4839\n",
      "Epoch 78/100\n",
      "62/62 [==============================] - 0s 390us/sample - loss: 1.1788 - acc: 0.5000\n",
      "Epoch 79/100\n",
      "62/62 [==============================] - 0s 649us/sample - loss: 1.1638 - acc: 0.5323\n",
      "Epoch 80/100\n",
      "62/62 [==============================] - 0s 634us/sample - loss: 1.1932 - acc: 0.5000\n",
      "Epoch 81/100\n",
      "62/62 [==============================] - 0s 503us/sample - loss: 1.1789 - acc: 0.5323\n",
      "Epoch 82/100\n",
      "62/62 [==============================] - 0s 623us/sample - loss: 1.1440 - acc: 0.5323\n",
      "Epoch 83/100\n",
      "62/62 [==============================] - 0s 558us/sample - loss: 1.1510 - acc: 0.5323\n",
      "Epoch 84/100\n",
      "62/62 [==============================] - 0s 547us/sample - loss: 1.1459 - acc: 0.5161\n",
      "Epoch 85/100\n",
      "62/62 [==============================] - 0s 563us/sample - loss: 1.1776 - acc: 0.5323\n",
      "Epoch 86/100\n",
      "62/62 [==============================] - 0s 493us/sample - loss: 1.1415 - acc: 0.5161\n",
      "Epoch 87/100\n",
      "62/62 [==============================] - 0s 547us/sample - loss: 1.1274 - acc: 0.5484\n",
      "Epoch 88/100\n",
      "62/62 [==============================] - 0s 466us/sample - loss: 1.1087 - acc: 0.5806\n",
      "Epoch 89/100\n",
      "62/62 [==============================] - 0s 560us/sample - loss: 1.1204 - acc: 0.5806\n",
      "Epoch 90/100\n",
      "62/62 [==============================] - 0s 473us/sample - loss: 1.0736 - acc: 0.5968\n",
      "Epoch 91/100\n",
      "62/62 [==============================] - 0s 450us/sample - loss: 1.0966 - acc: 0.5968\n",
      "Epoch 92/100\n",
      "62/62 [==============================] - 0s 499us/sample - loss: 1.0816 - acc: 0.5806\n",
      "Epoch 93/100\n",
      "62/62 [==============================] - 0s 434us/sample - loss: 1.0884 - acc: 0.5645\n",
      "Epoch 94/100\n",
      "62/62 [==============================] - 0s 479us/sample - loss: 1.0637 - acc: 0.5806\n",
      "Epoch 95/100\n",
      "62/62 [==============================] - 0s 487us/sample - loss: 1.0463 - acc: 0.6452\n",
      "Epoch 96/100\n",
      "62/62 [==============================] - 0s 564us/sample - loss: 1.0610 - acc: 0.5806\n",
      "Epoch 97/100\n",
      "62/62 [==============================] - 0s 591us/sample - loss: 1.0222 - acc: 0.6290\n",
      "Epoch 98/100\n",
      "62/62 [==============================] - 0s 466us/sample - loss: 1.0284 - acc: 0.6613\n",
      "Epoch 99/100\n",
      "62/62 [==============================] - 0s 449us/sample - loss: 1.0430 - acc: 0.6290\n",
      "Epoch 100/100\n",
      "62/62 [==============================] - 0s 498us/sample - loss: 1.0051 - acc: 0.6774\n",
      "2019-08-29 20:51:35 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.core.policies.keras_policy\u001b[0m  - Done fitting keras policy model\n",
      "2019-08-29 20:51:35 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.core.agent\u001b[0m  - Persisted model to '/tmp/tmps4_ueirk/core'\n",
      "\u001b[94mCore model training completed.\u001b[0m\n",
      "\u001b[94mTraining NLU model...\u001b[0m\n",
      "2019-08-29 20:51:35 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.nlu.training_data.training_data\u001b[0m  - Training data stats: \n",
      "\t- intent examples: 39 (6 distinct intents)\n",
      "\t- Found intents: 'affirm', 'deny', 'goodbye', 'greet', 'mood_great', 'mood_unhappy'\n",
      "\t- entity examples: 0 (0 distinct entities)\n",
      "\t- found entities: \n",
      "\n",
      "2019-08-29 20:51:35 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.nlu.model\u001b[0m  - Starting to train component WhitespaceTokenizer\n",
      "2019-08-29 20:51:35 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.nlu.model\u001b[0m  - Finished training component.\n",
      "2019-08-29 20:51:35 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.nlu.model\u001b[0m  - Starting to train component RegexFeaturizer\n",
      "2019-08-29 20:51:35 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.nlu.model\u001b[0m  - Finished training component.\n",
      "2019-08-29 20:51:35 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.nlu.model\u001b[0m  - Starting to train component CRFEntityExtractor\n",
      "2019-08-29 20:51:35 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.nlu.model\u001b[0m  - Finished training component.\n",
      "2019-08-29 20:51:35 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.nlu.model\u001b[0m  - Starting to train component EntitySynonymMapper\n",
      "2019-08-29 20:51:35 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.nlu.model\u001b[0m  - Finished training component.\n",
      "2019-08-29 20:51:35 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.nlu.model\u001b[0m  - Starting to train component CountVectorsFeaturizer\n",
      "2019-08-29 20:51:35 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.nlu.model\u001b[0m  - Finished training component.\n",
      "2019-08-29 20:51:35 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.nlu.model\u001b[0m  - Starting to train component EmbeddingIntentClassifier\n",
      "2019-08-29 20:51:36 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.nlu.classifiers.embedding_intent_classifier\u001b[0m  - Accuracy is updated every 10 epochs\n",
      "Epochs: 100%|██████████| 300/300 [00:03<00:00, 81.59it/s, loss=0.094, acc=1.000]\n",
      "2019-08-29 20:51:40 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.nlu.classifiers.embedding_intent_classifier\u001b[0m  - Finished training embedding classifier, loss=0.094, train accuracy=1.000\n",
      "2019-08-29 20:51:40 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.nlu.model\u001b[0m  - Finished training component.\n",
      "2019-08-29 20:51:40 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.nlu.model\u001b[0m  - Successfully saved model into '/tmp/tmps4_ueirk/nlu'\n",
      "\u001b[94mNLU model training completed.\u001b[0m\n",
      "\u001b[92mYour Rasa model is trained and saved at '/home/bimsara/Desktop/icter19/bot/models/20190829-205128.tar.gz'.\u001b[0m\n",
      "If you want to speak to the assistant, run 'rasa shell' at any time inside the project directory.\n"
     ]
    }
   ],
   "source": [
    "!rasa init --no-prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otiy7w8KY8gO"
   },
   "source": [
    "## 1. Natural Language Understanding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ln7WpHz_Y8gQ"
   },
   "source": [
    "In this section, you will enable your assistant to understand the user inputs by building a Rasa NLU model. This model will take unstructured user inputs and extract structured data in a form of intents and entities:  \n",
    "- *intent* - a label which represents the overall intention of the user 's input\n",
    "- *entity* - important detail which an assistant should extract and use to steer the conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jGmj13eJY8gS"
   },
   "source": [
    "### 1.1. Designing a happy path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lomSlZJJY8gU"
   },
   "source": [
    "A good starting point is to define a happy path first. A happy path is a conversation flow where the user provides all the required information and allows the assistant to lead the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jLHAPQJHY8gX"
   },
   "source": [
    "### 1.2. Designing the NLU training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wMvZPh_XY8gY"
   },
   "source": [
    "To train the NLU model you will need some labeled training data. Rasa NLU training data samples consist of the following components:  \n",
    "- intent label which starts with a prefix *\n",
    "- examples of text inputs which correspond to that label\n",
    "- entities which follow the format *[entity_value] (entity_label)*\n",
    "\n",
    "We will start by generating some training data examples by hand. For a completed data file check out the *helper_files/nlu_data.md* in the repository of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8c9n_YTxY8ga",
    "outputId": "e4f7d9c7-7c26-4963-88d5-fdc670ad6811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/nlu.md\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/nlu.md\n",
    "## intent:greet\n",
    "- Hello\n",
    "- hey\n",
    "- hello\n",
    "- heya\n",
    "- howdy\n",
    "- hello there\n",
    "- hi\n",
    "- hello there\n",
    "- good morning\n",
    "- good evening\n",
    "- moin\n",
    "- hey there\n",
    "- let's go\n",
    "- hey dude\n",
    "- goodmorning\n",
    "- goodevening\n",
    "- good afternoon\n",
    "\n",
    "## intent:goodbye\n",
    "- cu\n",
    "- good by\n",
    "- cee you later\n",
    "- good night\n",
    "- good afternoon\n",
    "- bye\n",
    "- goodbye\n",
    "- have a nice day\n",
    "- see you around\n",
    "- bye bye\n",
    "- see ya\n",
    "- see you later\n",
    "\n",
    "## intent:inform\n",
    "- What's the weather today?\n",
    "- What's the weather in [London](location) today?\n",
    "- Show me what's the weather in [Paris](location)\n",
    "- I wonder what is the weather in [Vilnius](location) right now?\n",
    "- what is the weather?\n",
    "- Tell me the weather\n",
    "- Is the weather nice in [Barcelona](location) today?\n",
    "- I am going to [London](location) today and I wonder what is the weather out there?\n",
    "- I am planning my trip to [Amsterdam](location). What is the weather out there?\n",
    "- Show me the weather in [Dublin](location), please\n",
    "- in [London](location)\n",
    "- [Lithuania](location)\n",
    "- Oh, sorry, in [Italy](location)\n",
    "- Tell me the weather in [Vilnius](location)\n",
    "- The weather condition in [Italy](location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vgzzQuO_Y8ge"
   },
   "source": [
    "\n",
    "## 1.3 Designing the training pipeline\n",
    "\n",
    "Once the training data is ready, we can define the NLU model. We can do that by constructing the processing pipeline which defines how structured data will be extracted from unstructured user inputs: how the sentences will be tokenized, what intent classifier will be used, what entity extraction model will be used, etc. Each component in a training pipeline is trained one after another and can take inputs from the previously defined component as well as pass some information to subsequent ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tjJNLeWgY8gf"
   },
   "outputs": [],
   "source": [
    "%%writefile config.yml\n",
    "# Configuration for Rasa NLU.\n",
    "# https://rasa.com/docs/rasa/nlu/components/\n",
    "language: \"en\"\n",
    "#pipeline: \"pretrained_embeddings_spacy\"\n",
    "pipeline:\n",
    "- name: \"SpacyNLP\"                    # loads the spacy language model\n",
    "- name: \"SpacyTokenizer\"              # splits the sentence into tokens\n",
    "- name: \"SpacyFeaturizer\"             # transform the sentence into a vector representation\n",
    "- name: \"RegexFeaturizer\"\n",
    "- name: \"CRFEntityExtractor\"\n",
    "- name: \"EntitySynonymMapper\"         # trains the synonyms\n",
    "- name: \"SklearnIntentClassifier\"     # uses the vector representation to classify using SVM\n",
    "\n",
    "# Configuration for Rasa Core.\n",
    "# https://rasa.com/docs/rasa/core/policies/\n",
    "policies:\n",
    "  - name: MemoizationPolicy\n",
    "  - name: KerasPolicy\n",
    "  - name: MappingPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c58NdY_RY8h1"
   },
   "source": [
    "# 2. Dialogue Management\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FyEqBxGVY8h2"
   },
   "source": [
    "In this section of this workshop you will build a machine learning-based dialogue model which will enable your assistant to decide on how to respond to user inputs based on the state of the conversation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cyAMbLsJY8h3"
   },
   "source": [
    "## 2.1 Designing the training stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YPqNmWcTY8h4"
   },
   "source": [
    "Let's start with generating the training data. Rasa Core models learn by observing real conversational data between the user and the assistant. The only important thing is that this data has to be converted into the Rasa Core format: user inputs have to be expressed as corresponding intents (and entities where necessary) while the responses of the assistant are expressed as action names. Each training story follows the format:  \n",
    "- the story starts with a story name which has a prefix ##  \n",
    "- intents, corresponding to user inputs, start with *  \n",
    "- if NLU model extracts entities which should influence the predictions of the dialogue model, they have to be included in the stories using the following format: * intent{'entity_name':\"entity_value\"}  \n",
    "- the responses of the bot start with -  \n",
    "- the story ends with an empty line which marks the end of the story\n",
    "\n",
    "In the next step of this tutorial, we will generate some training stories to cover the happy path. To see a complete training data example, check out the **data/stories.md** file of this repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hDbP8XPY8h5"
   },
   "outputs": [],
   "source": [
    "%%writefile data/stories.md\n",
    "## Generated Story 3320800183399695936\n",
    "* greet\n",
    "    - utter_greet\n",
    "* inform\n",
    "    - utter_ask_location\n",
    "* inform{\"location\": \"italy\"}\n",
    "    - slot{\"location\": \"italy\"}\n",
    "    - action_weather\n",
    "    - slot{\"location\": \"italy\"}\n",
    "* goodbye\n",
    "    - utter_goodbye\n",
    "    - export\n",
    "## Generated Story -3351152636827275381\n",
    "* greet\n",
    "    - utter_greet\n",
    "* inform{\"location\": \"London\"}\n",
    "    - slot{\"location\": \"London\"}\n",
    "    - action_weather\n",
    "* goodbye\n",
    "    - utter_goodbye\n",
    "    - export\n",
    "## Generated Story 8921121480760034253\n",
    "* greet\n",
    "    - utter_greet\n",
    "* inform\n",
    "    - utter_ask_location\n",
    "* inform{\"location\":\"London\"}\n",
    "    - slot{\"location\": \"London\"}\n",
    "    - action_weather\n",
    "* goodbye\n",
    "    - utter_goodbye\n",
    "    - export\n",
    "## Generated Story -5208991511085841103\n",
    "    - slot{\"location\": \"London\"}\n",
    "    - action_weather\n",
    "* goodbye\n",
    "    - utter_goodbye\n",
    "    - export\n",
    "## Generated Story -5208991511085841103\n",
    "    - slot{\"location\": \"London\"}\n",
    "    - action_weather\n",
    "* goodbye\n",
    "    - utter_goodbye\n",
    "    - export\n",
    "## story_001\n",
    "* greet\n",
    "   - utter_greet\n",
    "* inform\n",
    "   - utter_ask_location\n",
    "* inform{\"location\":\"London\"}\n",
    "   - slot{\"location\": \"London\"}\n",
    "   - action_weather\n",
    "* goodbye\n",
    "   - utter_goodbye\n",
    "## story_002\n",
    "* greet\n",
    "   - utter_greet\n",
    "* inform{\"location\":\"Paris\"}\n",
    "   - slot{\"location\": \"Paris\"}\n",
    "   - action_weather\n",
    "* goodbye\n",
    "   - utter_goodbye \n",
    "## story_003\n",
    "* greet\n",
    "   - utter_greet\n",
    "* inform\n",
    "   - utter_ask_location\n",
    "* inform{\"location\":\"Vilnius\"}\n",
    "   - slot{\"location\": \"Vilnius\"}\n",
    "   - action_weather\n",
    "* goodbye\n",
    "   - utter_goodbye\n",
    "## story_004\n",
    "* greet\n",
    "   - utter_greet\n",
    "* inform{\"location\":\"Italy\"}\n",
    "   - slot{\"location\": \"Italy\"}\n",
    "   - action_weather\n",
    "* goodbye\n",
    "   - utter_goodbye \n",
    "## story_005\n",
    "* greet\n",
    "   - utter_greet\n",
    "* inform\n",
    "   - utter_ask_location\n",
    "* inform{\"location\":\"Lithuania\"}\n",
    "   - slot{\"location\": \"Lithuania\"}\n",
    "   - action_weather\n",
    "* goodbye\n",
    "   - utter_goodbye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_d7jkssY8iB"
   },
   "source": [
    "### 2.2 Creating custom action\n",
    "\n",
    "We are going to use the backend integration to enable our assistant to fetch the relevant data based on user's queries. For that, we will create custom actions which, when predicted, will collect necessary data and use it to steer the conversation further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TbUx1oTNY8iD"
   },
   "outputs": [],
   "source": [
    "%%writefile actions.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "from rasa_sdk import Action\n",
    "from rasa_sdk.events import SlotSet\n",
    "\n",
    "class ActionWeather(Action):\n",
    "\tdef name(self):\n",
    "\t\treturn 'action_weather'\n",
    "\t\t\n",
    "\tdef run(self, dispatcher, tracker, domain):\n",
    "\t\tfrom apixu.client import ApixuClient\n",
    "\t\tapi_key = '' #your apixu key\n",
    "\t\tclient = ApixuClient(api_key)\n",
    "\t\t\n",
    "\t\tloc = tracker.get_slot('location')\n",
    "\t\tcurrent = client.getcurrent(q=loc)\n",
    "\t\t\n",
    "\t\tcountry = current['location']['country']\n",
    "\t\tcity = current['location']['name']\n",
    "\t\tcondition = current['current']['condition']['text']\n",
    "\t\ttemperature_c = current['current']['temp_c']\n",
    "\t\thumidity = current['current']['humidity']\n",
    "\t\twind_mph = current['current']['wind_mph']\n",
    "\n",
    "\t\tresponse = \"\"\"It is currently {} in {} at the moment. The temperature is {} degrees, the humidity is {}% and the wind speed is {} mph.\"\"\".format(condition, city, temperature_c, humidity, wind_mph)\n",
    "\t\t\t\t\t\t\n",
    "\t\tdispatcher.utter_message(response)\n",
    "\t\treturn [SlotSet('location',loc)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uUAojX1CY8iH"
   },
   "source": [
    "## 2.3 Defining the domain\n",
    "\n",
    "Once we have the training data in place, we can define the domain of our assistant. A domain defines the environment in which the assistant operates - what user inputs it should expect to see, what actions it should be able to predict, what information the assistant should store throughout the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y0yAXKlIY8iI"
   },
   "outputs": [],
   "source": [
    "%%writefile domain.yml\n",
    "slots:\n",
    "  location:\n",
    "    type: text\n",
    "\n",
    "\n",
    "intents:\n",
    " - greet\n",
    " - goodbye\n",
    " - inform\n",
    "\n",
    "\n",
    "entities:\n",
    " - location\n",
    "\n",
    "templates:\n",
    "  utter_greet:\n",
    "    - text: 'Hello! How can I help?'\n",
    "  utter_goodbye:\n",
    "    - text: 'Talk to you later.'\n",
    "    - text: 'Bye bye :('\n",
    "  utter_ask_location:\n",
    "    - text: 'In what location?'\n",
    "\n",
    "\n",
    "actions:\n",
    " - utter_greet\n",
    " - utter_goodbye\n",
    " - utter_ask_location\n",
    " - action_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GEyD-aF5Y8iL"
   },
   "source": [
    "## 3. Training and testing the model\n",
    "\n",
    "We now have all the components necessary to train our first model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Training the bot\n",
    "\n",
    "The code cell below will train the model using the defined pipeline and policies and store the model in a specified location for us to test later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0TaMe2n1Y8iY"
   },
   "outputs": [],
   "source": [
    "!rasa train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Testing the bot in the terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before testing the bot, first we need to start our **action server** in **a new terminal**. Following command will start the action server.\n",
    "\n",
    "**rasa run actions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to add the endpoint file to setup the connection between the action server and the rasa core server.\n",
    "\n",
    "**endpoints.yml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting endpoints.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile endpoints.yml\n",
    "action_endpoint:\n",
    "  url: \"http://localhost:5055/webhook\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following script will loads your trained model and lets you talk to your assistant on the command line **(You may need to run this in a new terminal)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**rasa shell**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k6fnBNS_Y8i6"
   },
   "source": [
    "## 3.3. Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rkP0XmvY8i7"
   },
   "source": [
    "Another great way to see how good our model is, is to test it using evaluation script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gs6H4epKY8i8"
   },
   "outputs": [],
   "source": [
    "!rasa test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rkP0XmvY8i7"
   },
   "source": [
    "You can visualize your training stories using the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gs6H4epKY8i8"
   },
   "outputs": [],
   "source": [
    "!rasa visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cLBlNGpQY8jA"
   },
   "source": [
    "# 4. Closing the feedback loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w6NpsceLY8jB"
   },
   "source": [
    "Developing an assistant is just one part of the process. Another very important part which defines a successful assistant is enabling your assistant to learn from real user feedback. In the last part of this workshop, we will cover two ways to improve your bots using real user feedback - using interactive learning and using the history of the conversations. We will also, connect our assistant to a custom webpage to see how it works in action! We will complete this part using the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c0OLu2JHY8jC"
   },
   "source": [
    "## 4.1. Improving the assistant using the interactive learning \n",
    "Interactive learning is a great way to improve your assistant and generate more training example by simply talking to your bot and providing feedback for all predictions it made. That is the main idea behind it - instead of responding right away, an assistant will tell you what it thinks it should do next and ask you for feedback. To start the interactive learning session, we will use a command line and use the following command:\n",
    "\n",
    "\n",
    "**rasa interactive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SaQbIxnaY8jF"
   },
   "source": [
    "# 5. Integrating the chatbot with Slack "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5F64rDOl90U"
   },
   "source": [
    "In the very last step of this workshop, you will learn how to connect your assistant to slack messaging platform. Rasa already supports many other messaging platforms and also allows you to implement your custom channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5F64rDOl90U"
   },
   "source": [
    "## 5.1. Setup the credentials\n",
    "You need to change **credentials.yml** files as bellow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**credentials.yml**\n",
    "\n",
    "To see the completed file, check out the helper_files/credentials.yml file of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting credentials.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile credentials.yml\n",
    "# This file contains the credentials for the voice & chat platforms\n",
    "# which your bot is using.\n",
    "# https://rasa.com/docs/rasa/user-guide/messaging-and-voice-channels/\n",
    "\n",
    "rest:\n",
    "#  # you don't need to provide anything here - this channel doesn't\n",
    "#  # require any credentials\n",
    "\n",
    "\n",
    "#facebook:\n",
    "#  verify: \"<verify>\"\n",
    "#  secret: \"<your secret>\"\n",
    "#  page-access-token: \"<your page access token>\"\n",
    "\n",
    "slack:\n",
    " slack_token: \"paste your slack token here (xoxb something something)\"\n",
    "#  slack_channel: \"<the slack channel>\"\n",
    "\n",
    "#socketio:\n",
    "#  user_message_evt: <event name for user message>\n",
    "#  bot_message_evt: <event name for but messages>\n",
    "#  session_persistence: <true/false>\n",
    "\n",
    "rasa:\n",
    "  url: \"http://localhost:5002/api\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5F64rDOl90U"
   },
   "source": [
    "## 5.2. Ngrok for local testing\n",
    "Ngrok is a multi-platform tunnelling, reverse proxy software that establishes secure tunnels from a public endpoint such as the internet to a locally running network service. In simple words it means, it opens access to your local app from the internet.\n",
    "\n",
    "You need to start a ngrok in **a new terminal**. Start it by telling it which port we want to expose to the public internet.\n",
    "\n",
    "**ngrok http 5005**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Start the rasa core server \n",
    "After setting up the backend, we will start our assistant on a server and connect to the UI using:\n",
    "\n",
    "**rasa run --endpoints endpoints.yml**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "GEyD-aF5Y8iL",
    "c0OLu2JHY8jC"
   ],
   "name": "Copy of Icter19_chatbotworkshop.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
